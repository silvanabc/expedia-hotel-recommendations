{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 671 ms, sys: 56 ms, total: 727 ms\n",
      "Wall time: 725 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ROWS = None\n",
    "ROWS = 100000\n",
    "\n",
    "df = pd.read_csv('data/train.csv.gz', \n",
    "                        nrows=ROWS,\n",
    "                        compression='gzip',\n",
    "                        error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4853, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = list(range(5))\n",
    "df = df[df.hotel_cluster.isin(clusters)].reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q(s, a, type=\"ones\"):\n",
    "    \"\"\"\n",
    "    @param s the number of states\n",
    "    @param a the number of actions\n",
    "    @param type random, ones or zeros for the initialization\n",
    "    \"\"\"\n",
    "    if type == \"ones\":\n",
    "        return np.ones((s, a))\n",
    "    elif type == \"random\":\n",
    "        return np.random.random((s, a))\n",
    "    elif type == \"zeros\":\n",
    "        return np.zeros((s, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
    "    \"\"\"\n",
    "    @param Q Q values state x action -> value\n",
    "    @param epsilon for exploration\n",
    "    @param s number of states\n",
    "    @param train if true then no random actions selected\n",
    "    \"\"\"\n",
    "    if train or np.random.rand() < epsilon:\n",
    "        action = np.argmax(Q[s, :])\n",
    "    else:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(df, idx, action):\n",
    "    observation = df.iloc[idx]['hotel_cluster']\n",
    "    reward = -1\n",
    "    \n",
    "    if(observation == action):\n",
    "        reward = 1\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.4       ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.4       ,  0.4       ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.4       ,  0.79984   ,  0.        ,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-1.45249369, -1.48742386, -1.68192312,  0.209811  , -1.57908836],\n",
       "       [-1.38969725, -1.56971863, -1.56785915, -0.1065511 , -1.55550692],\n",
       "       [-1.        , -1.        , -1.        , -1.        , -1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
    "\"\"\"\n",
    "@param alpha learning rate\n",
    "@param gamma decay factor\n",
    "@param epsilon for exploration\n",
    "@param max_steps for max step in each episode\n",
    "@param n_tests number of test episodes\n",
    "\"\"\"\n",
    "#===PARAMS===#\n",
    "alpha = 0.4\n",
    "gamma = 0.999\n",
    "epsilon = 0.8\n",
    "episodes = 10#10000\n",
    "max_steps = 50#2500\n",
    "n_tests = 2\n",
    "#=============#\n",
    "\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "n_states = df.shape[0]\n",
    "actions = np.sort(df.hotel_cluster.unique()) #hotel_clusters\n",
    "n_actions = actions.size\n",
    "\n",
    "Q = init_q(n_states, n_actions, type=\"zeros\")\n",
    "\n",
    "\n",
    "timestep_reward = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    s = idx\n",
    "    a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
    "    t = 0\n",
    "    total_reward = 0\n",
    "    done = False #check if it is the last row \n",
    "    \n",
    "    if (idx % 100 == 0):\n",
    "        print(idx)\n",
    "    \n",
    "    \n",
    "    while t < max_steps:\n",
    "        t += 1\n",
    "        reward = step(df, idx, a)\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        done = (s == df.shape[0]-1)\n",
    "        \n",
    "        if done:\n",
    "            Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
    "        else:\n",
    "            s_ = s + 1\n",
    "            a_ = np.argmax(Q[s_, :])\n",
    "            Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
    "        s, a = s_, a_\n",
    "Q      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[-0.4         0.          0.          0.          0.        ]\n",
      " [-0.4         0.4         0.          0.          0.        ]\n",
      " [-0.4         0.79984     0.          0.          0.        ]\n",
      " ...\n",
      " [-1.45249369 -1.48742386 -1.68192312  0.209811   -1.57908836]\n",
      " [-1.38969725 -1.56971863 -1.56785915 -0.1065511  -1.55550692]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Qlearning is an off policy learning python implementation.\n",
    "This is a python implementation of the qlearning algorithm in the Sutton and\n",
    "Barto's book on RL. It's called SARSA because - (state, action, reward, state,\n",
    "action). The only difference between SARSA and Qlearning is that SARSA takes the\n",
    "next action based on the current policy while qlearning takes the action with\n",
    "maximum utility of next state.\n",
    "Using the simplest gym environment for brevity: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\"\"\"\n",
    "\n",
    "def init_q(s, a, type=\"ones\"):\n",
    "    \"\"\"\n",
    "    @param s the number of states\n",
    "    @param a the number of actions\n",
    "    @param type random, ones or zeros for the initialization\n",
    "    \"\"\"\n",
    "    if type == \"ones\":\n",
    "        return np.ones((s, a))\n",
    "    elif type == \"random\":\n",
    "        return np.random.random((s, a))\n",
    "    elif type == \"zeros\":\n",
    "        return np.zeros((s, a))\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
    "    \"\"\"\n",
    "    @param Q Q values state x action -> value\n",
    "    @param epsilon for exploration\n",
    "    @param s number of states\n",
    "    @param train if true then no random actions selected\n",
    "    \"\"\"\n",
    "    if train or np.random.rand() < epsilon:\n",
    "        action = np.argmax(Q[s, :])\n",
    "    else:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    return action\n",
    "\n",
    "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
    "    \"\"\"\n",
    "    @param alpha learning rate\n",
    "    @param gamma decay factor\n",
    "    @param epsilon for exploration\n",
    "    @param max_steps for max step in each episode\n",
    "    @param n_tests number of test episodes\n",
    "    \"\"\"\n",
    "    env = gym.make('Taxi-v2')\n",
    "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
    "    timestep_reward = []\n",
    "    for episode in range(episodes):\n",
    "        print(f\"Episode: {episode}\")\n",
    "        s = env.reset()\n",
    "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
    "        t = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while t < max_steps:\n",
    "            if render:\n",
    "                env.render()\n",
    "            t += 1\n",
    "            s_, reward, done, info = env.step(a)\n",
    "            total_reward += reward\n",
    "            a_ = np.argmax(Q[s_, :])\n",
    "            if done:\n",
    "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
    "            else:\n",
    "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
    "            s, a = s_, a_\n",
    "            if done:\n",
    "                if render:\n",
    "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
    "                timestep_reward.append(total_reward)\n",
    "                break\n",
    "    if render:\n",
    "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
    "    if test:\n",
    "        test_agent(Q, env, n_tests, n_actions)\n",
    "    return timestep_reward\n",
    "\n",
    "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
    "    for test in range(n_tests):\n",
    "        print(f\"Test #{test}\")\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        epsilon = 0\n",
    "        while True:\n",
    "            time.sleep(delay)\n",
    "            env.render()\n",
    "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
    "            print(f\"Chose action {a} for state {s}\")\n",
    "            s, reward, done, info = env.step(a)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    print(\"Reached goal!\")\n",
    "                else:\n",
    "                    print(\"Shit! dead x_x\")\n",
    "                time.sleep(3)\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    alpha = 0.4\n",
    "    gamma = 0.999\n",
    "    epsilon = 0.9\n",
    "    episodes = 10000\n",
    "    max_steps = 2500\n",
    "    n_tests = 2\n",
    "    timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
    "    print(timestep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000 #size of the dataset\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.5425\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[1.99621167e-01 5.89175006e-03 8.07100994e-03 6.04850852e-03]\n",
      " [4.18034281e-03 2.74940669e-03 1.43662286e-06 1.86893913e-01]\n",
      " [6.03207088e-04 1.70233402e-03 7.81149229e-03 1.94338505e-01]\n",
      " [4.71433751e-04 6.44633989e-04 4.89921832e-04 5.12625796e-02]\n",
      " [2.08005473e-01 1.54144113e-04 2.39960635e-03 3.57595370e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.07535346e-05 3.71499356e-05 1.04479335e-01 6.62555811e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.75994800e-03 0.00000000e+00 1.02259355e-03 1.99179271e-01]\n",
      " [0.00000000e+00 5.22050616e-01 0.00000000e+00 0.00000000e+00]\n",
      " [1.40918714e-01 0.00000000e+00 7.38060901e-04 1.50394224e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.47850461e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.58250879e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
